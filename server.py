import grpc
from concurrent import futures
import categorizer_pb2
import categorizer_pb2_grpc
import fasttext
import psycopg2
import psycopg2.extras
import os
import re
import logging
import tempfile
import hashlib
import io
from datetime import datetime
from threading import Lock
import time
import json
import threading

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DB_URL = os.getenv('DATABASE_URL', 'postgres://user:pass@db:5432/mydb')

class DatabaseReader:
    def __init__(self, db_url):
        self.db_url = db_url
        self.last_trained_at = datetime(1970, 1, 1)  # –í –ø–∞–º—è—Ç–∏, –Ω–µ –≤ —Ñ–∞–π–ª–µ
    
    def _get_conn(self):
        return psycopg2.connect(self.db_url)
    
    def get_all_categories(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
        conn = self._get_conn()
        try:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT id, name, icon, color 
                    FROM categories 
                    ORDER BY name
                """)
                categories = []
                for row in cur.fetchall():
                    cat = dict(row)
                    cur.execute("""
                        SELECT text, created_at FROM examples 
                        WHERE category_id = %s
                    """, (cat['id'],))
                    rows = cur.fetchall()
                    cat['examples'] = [r['text'] for r in rows]
                    cat['created_ats'] = [r['created_at'] for r in rows]
                    categories.append(cat)
                return categories
        finally:
            conn.close()
    
    def get_examples_count_since(self, since: datetime) -> int:
        """–°–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
        conn = self._get_conn()
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM examples WHERE created_at > %s", (since,))
                return cur.fetchone()[0]
        finally:
            conn.close()


class FastTextCategorizerServicer:
    def __init__(self):
        self.db = DatabaseReader(DB_URL)
        self.model = None
        self.is_training = False
        self.training_lock = Lock()
        self.categories_cache = []
        self.training_data = []  # –•—Ä–∞–Ω–∏–º –≤ –ø–∞–º—è—Ç–∏ –≤–º–µ—Å—Ç–æ —Ñ–∞–π–ª–∞
        self.last_trained_hash = set()  # –•–µ—à–∏ —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.lr = 0.5
        self.word_ngrams = 2
        self.dim = 25
        self.epoch = 25
        self.bucket = 100000
        self.incremental_epoch = 5
        self.thread = 1
        
        self._init_model()
        self._start_watcher()
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text or not isinstance(text, str):
            return ""
        text = text.lower().strip()
        text = re.sub(r'\d+[\s]*[‚ÇΩ—Ä—É–±$‚Ç¨]?', '', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        return ' '.join(text.split())
    
    def _generate_training_lines(self, categories) -> list[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ –æ–±—É—á–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç–∏"""
        lines = []
        for cat in categories:
            for example in cat.get('examples', []):
                clean = self._clean_text(example)
                if clean:
                    line = f"__label__{cat['id']} {clean}"
                    lines.append(line)
        return lines
    
    def _train_model_from_lines(self, lines: list[str], epoch: int = None) -> bool:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫ –≤ –ø–∞–º—è—Ç–∏ (–±–µ–∑ —Ñ–∞–π–ª–æ–≤)"""
        if not lines:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False
        
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Ç–æ–ª—å–∫–æ –¥–ª—è FastText (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Ç—Ä–µ–±—É–µ—Ç —Ñ–∞–π–ª)
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            f.write('\n'.join(lines))
            temp_path = f.name
        
        try:
            self.model = fasttext.train_supervised(
                input=temp_path,
                lr=self.lr,
                epoch=epoch or self.epoch,
                wordNgrams=self.word_ngrams,
                bucket=self.bucket,
                thread=self.thread,
                dim=self.dim,
                loss='softmax'
            )
            return True
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            try:
                os.unlink(temp_path)
            except:
                pass
    
    def _full_train(self) -> bool:
        """–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with self.training_lock:
            self.is_training = True
            try:
                categories = self.db.get_all_categories()
                if not categories:
                    logger.warning("‚ö†Ô∏è –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ë–î!")
                    return False
                
                lines = self._generate_training_lines(categories)
                if not lines:
                    logger.error("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
                    return False
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ö–µ—à–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
                self.training_data = lines
                self.last_trained_hash = {hashlib.md5(line.encode()).hexdigest() for line in lines}
                
                # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–∞—Ç—É —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞
                max_created = datetime(1970, 1, 1)
                for cat in categories:
                    for created_at in cat.get('created_ats', []):
                        if created_at and created_at > max_created:
                            max_created = created_at
                
                logger.info(f"üìö –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: {len(lines)} –ø—Ä–∏–º–µ—Ä–æ–≤, {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                
                success = self._train_model_from_lines(lines, self.epoch)
                if success:
                    self.categories_cache = categories
                    self.db.last_trained_at = max_created if max_created != datetime(1970, 1, 1) else datetime.now()
                    logger.info(f"‚úÖ –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, last_trained_at: {self.db.last_trained_at}")
                
                return success
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: {e}")
                import traceback
                traceback.print_exc()
                return False
            finally:
                self.is_training = False
    
    def _incremental_train(self) -> bool:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with self.training_lock:
            self.is_training = True
            try:
                categories = self.db.get_all_categories()
                if not categories:
                    logger.warning("‚ö†Ô∏è –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ë–î!")
                    return False
                
                # –ü–æ–ª—É—á–∞–µ–º –í–°–ï –ø—Ä–∏–º–µ—Ä—ã, –Ω–æ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –ø–æ —Ö–µ—à—É
                all_lines = self._generate_training_lines(categories)
                new_lines = []
                max_created = self.db.last_trained_at
                
                for cat in categories:
                    for example, created_at in zip(cat.get('examples', []), cat.get('created_ats', [])):
                        clean = self._clean_text(example)
                        if clean:
                            line = f"__label__{cat['id']} {clean}"
                            line_hash = hashlib.md5(line.encode()).hexdigest()
                            if line_hash not in self.last_trained_hash:
                                new_lines.append(line)
                                self.last_trained_hash.add(line_hash)
                                # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–∞—Ç—É
                                if created_at and created_at > max_created:
                                    max_created = created_at
                
                if not new_lines:
                    logger.info("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                    # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–Ω–æ–≤–∞
                    self.db.last_trained_at = datetime.now()
                    return False
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –∏ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                combined_lines = self.training_data + new_lines
                self.training_data = combined_lines
                
                logger.info(f"üìà –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: {len(new_lines)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–≤—Å–µ–≥–æ: {len(combined_lines)})")
                
                success = self._train_model_from_lines(combined_lines, self.incremental_epoch)
                if success:
                    self.categories_cache = categories
                    self.db.last_trained_at = max_created if max_created > self.db.last_trained_at else datetime.now()
                    logger.info(f"‚úÖ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, last_trained_at: {self.db.last_trained_at}")
                
                return success
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: {e}")
                import traceback
                traceback.print_exc()
                return False
            finally:
                self.is_training = False
    
    def _init_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        # –í—Å–µ–≥–¥–∞ –¥–µ–ª–∞–µ–º –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏)
        logger.info("üÜï –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        success = self._full_train()
        if not success:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ!")
            # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç—É—é –º–æ–¥–µ–ª—å-–∑–∞–≥–ª—É—à–∫—É, —á—Ç–æ–±—ã —Å–µ—Ä–≤–µ—Ä –Ω–µ —É–ø–∞–ª
            self._create_dummy_model()
    
    def _create_dummy_model(self):
        """–°–æ–∑–¥–∞—ë—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã —Å–µ—Ä–≤–µ—Ä –º–æ–≥ —Ä–∞–±–æ—Ç–∞—Ç—å"""
        try:
            dummy_lines = ["__label__1 test example"]
            self._train_model_from_lines(dummy_lines, 1)
            logger.warning("‚ö†Ô∏è –°–æ–∑–¥–∞–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞ –º–æ–¥–µ–ª–∏")
        except Exception as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∑–∞–≥–ª—É—à–∫—É: {e}")
    
    def _start_watcher(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑ –≤ 30 —Å–µ–∫—É–Ω–¥"""
        def watch():
            # –ñ–¥—ë–º –ø–µ—Ä–≤–∏—á–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            time.sleep(5)
            while True:
                time.sleep(30)
                try:
                    if self.is_training:
                        continue
                    
                    new_count = self.db.get_examples_count_since(self.db.last_trained_at)
                    if new_count > 0:
                        logger.info(f"üîÑ Watcher: {new_count} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
                        self._incremental_train()
                    else:
                        logger.debug("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ watcher: {e}")
        
        threading.Thread(target=watch, daemon=True).start()
        logger.info("üëÅÔ∏è Watcher –∑–∞–ø—É—â–µ–Ω (–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 30—Å)")
    
    # ============ gRPC –º–µ—Ç–æ–¥—ã ============
    
    def Predict(self, request, context):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if self.is_training:
            context.set_code(grpc.StatusCode.UNAVAILABLE)
            context.set_details("–ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è, –ø–æ–¥–æ–∂–¥–∏—Ç–µ")
            return categorizer_pb2.PredictResponse()
        
        if not self.model:
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return categorizer_pb2.PredictResponse()
        
        clean = self._clean_text(request.text)
        if not clean:
            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
            context.set_details("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
            return categorizer_pb2.PredictResponse()
        
        try:
            labels, probs = self.model.predict(clean, k=3)
            
            alternatives = []
            for label, prob in zip(labels, probs):
                cat_id = int(label.replace('__label__', ''))
                
                cat_meta = next(
                    (c for c in self.categories_cache if c['id'] == cat_id),
                    {'name': str(cat_id), 'icon': '‚ùì', 'color': '#CCCCCC'}
                )
                
                alternatives.append(categorizer_pb2.PredictionResult(
                    category_id=str(cat_id),
                    category_name=cat_meta['name'],
                    category_icon=cat_meta['icon'],
                    category_color=cat_meta['color'],
                    confidence=float(prob)
                ))
            
            primary = alternatives[0] if alternatives else None
            
            return categorizer_pb2.PredictResponse(
                primary=primary,
                alternatives=alternatives[1:],
                needs_confirmation=(primary.confidence < 0.7) if primary else True,
                source='fasttext'
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return categorizer_pb2.PredictResponse()
    
    def ForceRetrain(self, request, context):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        force_full = request.full
        
        if force_full:
            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ö–µ—à–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            self.last_trained_hash = set()
            self.training_data = []
            self.db.last_trained_at = datetime(1970, 1, 1)
            success = self._full_train()
            msg = "–ü–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ" if success else "–û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
        else:
            success = self._incremental_train()
            msg = "–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ" if success else "–ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
        
        return categorizer_pb2.StatusResponse(
            success=success,
            message=msg,
            categories_count=len(self.categories_cache),
            is_training=self.is_training
        )
    
    def GetStatus(self, request, context):
        """–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return categorizer_pb2.StatusResponse(
            success=True,
            message="–°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç",
            categories_count=len(self.categories_cache),
            is_training=self.is_training
        )
    
    def GetModelInfo(self, request, context):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
        info = {
            'last_trained_at': self.db.last_trained_at.isoformat(),
            'examples_count': len(self.training_data),
            'categories_count': len(self.categories_cache),
            'unique_hashes': len(self.last_trained_hash),
            'params': {
                'lr': self.lr,
                'epoch': self.epoch,
                'wordNgrams': self.word_ngrams,
                'dim': self.dim
            }
        }
        
        return categorizer_pb2.ModelInfoResponse(
            model_path='in-memory',
            data_hash=str(len(self.last_trained_hash)),
            categories_count=len(self.categories_cache),
            is_training=self.is_training,
            metadata=json.dumps(info)
        )


def serve():
    """–ó–∞–ø—É—Å–∫ gRPC —Å–µ—Ä–≤–µ—Ä–∞"""
    port = os.getenv('PORT', '50051')
    
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    
    servicer = FastTextCategorizerServicer()
    categorizer_pb2_grpc.add_ExpenseCategorizerServicer_to_server(servicer, server)
    
    server.add_insecure_port(f'0.0.0.0:{port}')
    server.start()
    
    logger.info(f"üöÄ gRPC —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    logger.info(f"üìä PostgreSQL: {DB_URL.replace('pass', '***') if 'pass' in DB_URL else '***'}")
    
    server.wait_for_termination()


if __name__ == '__main__':
    serve()