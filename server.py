import fasttext
import psycopg2
import psycopg2.extras
import os
import re
import logging
import tempfile
import hashlib
from datetime import datetime
from threading import Lock
import time
import json
import threading
from flask import Flask, request, jsonify

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DB_URL = os.getenv('DATABASE_URL', 'postgres://user:pass@db:5432/mydb')
PORT = int(os.getenv('PORT', '8080'))

app = Flask(__name__)


class DatabaseReader:
    def __init__(self, db_url):
        self.db_url = db_url
        self.last_trained_at = datetime(1970, 1, 1)
    
    def _get_conn(self):
        return psycopg2.connect(self.db_url)
    
    def get_all_categories(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
        conn = self._get_conn()
        try:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cur:
                cur.execute("""
                    SELECT id, name, icon, color 
                    FROM categories 
                    ORDER BY name
                """)
                categories = []
                for row in cur.fetchall():
                    cat = dict(row)
                    cur.execute("""
                        SELECT text, created_at FROM examples 
                        WHERE category_id = %s
                    """, (cat['id'],))
                    rows = cur.fetchall()
                    cat['examples'] = [r['text'] for r in rows]
                    cat['created_ats'] = [r['created_at'] for r in rows]
                    categories.append(cat)
                return categories
        finally:
            conn.close()
    
    def get_examples_count_since(self, since: datetime) -> int:
        """–°–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤"""
        conn = self._get_conn()
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM examples WHERE created_at > %s", (since,))
                return cur.fetchone()[0]
        finally:
            conn.close()


class CategorizerService:
    def __init__(self):
        self.db = DatabaseReader(DB_URL)
        self.model = None
        self.is_training = False
        self.training_lock = Lock()
        self.categories_cache = []
        self.training_data = []
        self.last_trained_hash = set()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã FastText
        self.lr = 0.5
        self.word_ngrams = 2
        self.dim = 25
        self.epoch = 25
        self.bucket = 100000
        self.incremental_epoch = 5
        self.thread = 1
        
        self._init_model()
        self._start_watcher()
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not text or not isinstance(text, str):
            return ""
        text = text.lower().strip()
        text = re.sub(r'\d+[\s]*[‚ÇΩ—Ä—É–±$‚Ç¨]?', '', text)
        text = re.sub(r'[^\w\s]', ' ', text)
        return ' '.join(text.split())
    
    def _generate_training_lines(self, categories) -> list[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ –æ–±—É—á–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç–∏"""
        lines = []
        for cat in categories:
            for example in cat.get('examples', []):
                clean = self._clean_text(example)
                if clean:
                    line = f"__label__{cat['id']} {clean}"
                    lines.append(line)
        return lines
    
    def _train_model_from_lines(self, lines: list[str], epoch: int = None) -> bool:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫ –≤ –ø–∞–º—è—Ç–∏"""
        if not lines:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            f.write('\n'.join(lines))
            temp_path = f.name
        
        try:
            self.model = fasttext.train_supervised(
                input=temp_path,
                lr=self.lr,
                epoch=epoch or self.epoch,
                wordNgrams=self.word_ngrams,
                bucket=self.bucket,
                thread=self.thread,
                dim=self.dim,
                loss='softmax'
            )
            return True
        finally:
            try:
                os.unlink(temp_path)
            except:
                pass
    
    def _full_train(self) -> bool:
        """–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with self.training_lock:
            self.is_training = True
            try:
                categories = self.db.get_all_categories()
                if not categories:
                    logger.warning("‚ö†Ô∏è –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ë–î!")
                    return False
                
                lines = self._generate_training_lines(categories)
                if not lines:
                    logger.error("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
                    return False
                
                self.training_data = lines
                self.last_trained_hash = {hashlib.md5(line.encode()).hexdigest() for line in lines}
                
                max_created = datetime(1970, 1, 1)
                for cat in categories:
                    for created_at in cat.get('created_ats', []):
                        if created_at and created_at > max_created:
                            max_created = created_at
                
                logger.info(f"üìö –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: {len(lines)} –ø—Ä–∏–º–µ—Ä–æ–≤, {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                
                success = self._train_model_from_lines(lines, self.epoch)
                if success:
                    self.categories_cache = categories
                    self.db.last_trained_at = max_created if max_created != datetime(1970, 1, 1) else datetime.now()
                    logger.info(f"‚úÖ –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, last_trained_at: {self.db.last_trained_at}")
                
                return success
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: {e}")
                import traceback
                traceback.print_exc()
                return False
            finally:
                self.is_training = False
    
    def _incremental_train(self) -> bool:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        with self.training_lock:
            self.is_training = True
            try:
                categories = self.db.get_all_categories()
                if not categories:
                    logger.warning("‚ö†Ô∏è –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ë–î!")
                    return False
                
                all_lines = self._generate_training_lines(categories)
                new_lines = []
                max_created = self.db.last_trained_at
                
                for cat in categories:
                    for example, created_at in zip(cat.get('examples', []), cat.get('created_ats', [])):
                        clean = self._clean_text(example)
                        if clean:
                            line = f"__label__{cat['id']} {clean}"
                            line_hash = hashlib.md5(line.encode()).hexdigest()
                            if line_hash not in self.last_trained_hash:
                                new_lines.append(line)
                                self.last_trained_hash.add(line_hash)
                                if created_at and created_at > max_created:
                                    max_created = created_at
                
                if not new_lines:
                    logger.info("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                    self.db.last_trained_at = datetime.now()
                    return False
                
                combined_lines = self.training_data + new_lines
                self.training_data = combined_lines
                
                logger.info(f"üìà –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: {len(new_lines)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–≤—Å–µ–≥–æ: {len(combined_lines)})")
                
                success = self._train_model_from_lines(combined_lines, self.incremental_epoch)
                if success:
                    self.categories_cache = categories
                    self.db.last_trained_at = max_created if max_created > self.db.last_trained_at else datetime.now()
                    logger.info(f"‚úÖ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, last_trained_at: {self.db.last_trained_at}")
                
                return success
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: {e}")
                import traceback
                traceback.print_exc()
                return False
            finally:
                self.is_training = False
    
    def _init_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        logger.info("üÜï –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ...")
        success = self._full_train()
        if not success:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ!")
            self._create_dummy_model()
    
    def _create_dummy_model(self):
        """–°–æ–∑–¥–∞—ë—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã —Å–µ—Ä–≤–µ—Ä –º–æ–≥ —Ä–∞–±–æ—Ç–∞—Ç—å"""
        try:
            dummy_lines = ["__label__1 test example"]
            self._train_model_from_lines(dummy_lines, 1)
            logger.warning("‚ö†Ô∏è –°–æ–∑–¥–∞–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∑–∞–≥–ª—É—à–∫–∞ –º–æ–¥–µ–ª–∏")
        except Exception as e:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∑–∞–≥–ª—É—à–∫—É: {e}")
    
    def _start_watcher(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑ –≤ 30 —Å–µ–∫—É–Ω–¥"""
        def watch():
            time.sleep(5)
            while True:
                time.sleep(30)
                try:
                    if self.is_training:
                        continue
                    
                    new_count = self.db.get_examples_count_since(self.db.last_trained_at)
                    if new_count > 0:
                        logger.info(f"üîÑ Watcher: {new_count} –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
                        self._incremental_train()
                        
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ watcher: {e}")
        
        threading.Thread(target=watch, daemon=True).start()
        logger.info("üëÅÔ∏è Watcher –∑–∞–ø—É—â–µ–Ω (–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 30—Å)")
    
    def predict(self, text: str) -> dict:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if self.is_training:
            return {
                'success': False,
                'error': '–ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è, –ø–æ–¥–æ–∂–¥–∏—Ç–µ',
                'is_training': True
            }
        
        if not self.model:
            return {
                'success': False,
                'error': '–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'
            }
        
        clean = self._clean_text(text)
        if not clean:
            return {
                'success': False,
                'error': '–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç'
            }
        
        try:
            labels, probs = self.model.predict(clean, k=3)
            
            alternatives = []
            for label, prob in zip(labels, probs):
                cat_id = int(label.replace('__label__', ''))
                
                cat_meta = next(
                    (c for c in self.categories_cache if c['id'] == cat_id),
                    {'name': str(cat_id), 'icon': '‚ùì', 'color': '#CCCCCC'}
                )
                
                alternatives.append({
                    'category_id': str(cat_id),
                    'category_name': cat_meta['name'],
                    'category_icon': cat_meta['icon'],
                    'category_color': cat_meta['color'],
                    'confidence': float(prob)
                })
            
            primary = alternatives[0] if alternatives else None
            
            return {
                'success': True,
                'primary': primary,
                'alternatives': alternatives[1:],
                'needs_confirmation': (primary['confidence'] < 0.7) if primary else True,
                'source': 'fasttext'
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def force_retrain(self, full: bool = False) -> dict:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        if full:
            self.last_trained_hash = set()
            self.training_data = []
            self.db.last_trained_at = datetime(1970, 1, 1)
            success = self._full_train()
            msg = "–ü–æ–ª–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ" if success else "–û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
        else:
            success = self._incremental_train()
            msg = "–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ" if success else "–ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
        
        return {
            'success': success,
            'message': msg,
            'categories_count': len(self.categories_cache),
            'is_training': self.is_training
        }
    
    def get_status(self) -> dict:
        """–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            'success': True,
            'message': '–°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç',
            'categories_count': len(self.categories_cache),
            'is_training': self.is_training
        }
    
    def get_model_info(self) -> dict:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
        return {
            'success': True,
            'last_trained_at': self.db.last_trained_at.isoformat(),
            'examples_count': len(self.training_data),
            'categories_count': len(self.categories_cache),
            'unique_hashes': len(self.last_trained_hash),
            'is_training': self.is_training,
            'params': {
                'lr': self.lr,
                'epoch': self.epoch,
                'wordNgrams': self.word_ngrams,
                'dim': self.dim
            }
        }


# –°–æ–∑–¥–∞—ë–º —Å–µ—Ä–≤–∏—Å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
service = CategorizerService()


# ============ HTTP Endpoints ============

@app.route('/health', methods=['GET'])
def health():
    """Health check –¥–ª—è Render/Fly.io"""
    return jsonify({'status': 'ok', 'is_training': service.is_training})


@app.route('/predict', methods=['POST'])
def predict():
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    data = request.get_json()
    if not data or 'text' not in data:
        return jsonify({'success': False, 'error': '–ü–æ–ª–µ text –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ'}), 400
    
    result = service.predict(data['text'])
    status_code = 200 if result.get('success') else (503 if result.get('is_training') else 500)
    return jsonify(result), status_code


@app.route('/retrain', methods=['POST'])
def retrain():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
    data = request.get_json() or {}
    result = service.force_retrain(full=data.get('full', False))
    return jsonify(result)


@app.route('/status', methods=['GET'])
def status():
    """–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
    return jsonify(service.get_status())


@app.route('/model-info', methods=['GET'])
def model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    return jsonify(service.get_model_info())


@app.route('/categories', methods=['GET'])
def get_categories():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    return jsonify({
        'success': True,
        'categories': [
            {
                'id': c['id'],
                'name': c['name'],
                'icon': c['icon'],
                'color': c['color']
            }
            for c in service.categories_cache
        ]
    })


if __name__ == '__main__':
    logger.info(f"üöÄ HTTP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞ –ø–æ—Ä—Ç—É {PORT}")
    # –î–ª—è production –∏—Å–ø–æ–ª—å–∑—É–µ–º threaded=True, –¥–ª—è Render/Fly.io —ç—Ç–æ –≤–∞–∂–Ω–æ
    app.run(host='0.0.0.0', port=PORT, threaded=True)